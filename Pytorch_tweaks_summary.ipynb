{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PavUWDvgzCAr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch LLM Architecture & Data Flow — Simple, Intuitive Guide\n",
        "\n",
        "Main takeaway: Modern LLMs are just stacks of smart “attention + MLP” blocks. You can swap attention styles, tweak feed-forward layers, add small adapters, and optimize memory/training to get big gains without changing the whole model. Below are the concepts explained simply with practical examples and when to use them.\n",
        "\n",
        "***\n",
        "\n",
        "## Custom Transformer Components\n",
        "\n",
        "### Multi-Head Attention Variants\n",
        "\n",
        "[check out - Attention visualization](https://www.youtube.com/watch?app=desktop&v=RNF0FvRjGZk)\n",
        "\n",
        "Think of attention as “looking up relevant info” across the sequence.\n",
        "\n",
        "- Multi-Query Attention (MQA): All attention heads share the same Key/Value, but have different Queries.\n",
        "  - Why: Saves memory/bandwidth at inference time; faster decoding.\n",
        "  - Analogy: Many people asking questions (queries), but the library of facts (key/values) is shared.\n",
        "  - Use when: You need faster generation (server-side), minimal quality loss vs full MHA.\n",
        "\n",
        "- Grouped Query Attention (GQA): Queries are grouped; each group shares K/V.\n",
        "  - Why: Middle ground between full MHA and MQA—better quality than MQA, cheaper than MHA.\n",
        "  - Use when: You want speed-ups with less accuracy drop than MQA.\n",
        "\n",
        "- FlexAttention (PyTorch 2.5+): A flexible attention API where you can plug custom scores, masks, or biases.\n",
        "  - Why: Experiment quickly (e.g., penalize repeated tokens, prioritize recency).\n",
        "  - Example: Add a bias to prefer recent tokens in long-context chat; or block attention across segments.\n",
        "\n",
        "Intuition example: Summarizing a story. MHA = each head looks for different patterns (characters, places, chronology). MQA/GQA: fewer libraries to consult; answers faster. FlexAttention: you can say “focus more on the ending” with a custom score modifier.\n",
        "\n",
        "***\n",
        "\n",
        "### Advanced Feed-Forward Networks (FFN)\n",
        "The MLP between attentions shapes the information.\n",
        "\n",
        "- SwiGLU (used in LLaMA/PaLM): A gated activation replacing ReLU/GELU that often improves performance at similar cost.\n",
        "  - Intuition: A smarter filter that lets useful signals pass more cleanly.\n",
        "  - Use when: You want accuracy bump without major changes.\n",
        "\n",
        "- Mixture of Experts (MoE): Many expert MLPs; a router picks a couple per token.\n",
        "  - Why: Scales parameter count without proportional compute per token.\n",
        "  - Analogy: A call center routes each call to the best specialist.\n",
        "  - Use when: You need larger capacity with controlled FLOPs.\n",
        "\n",
        "***\n",
        "\n",
        "## Positional Encodings\n",
        "Models need to know token order.\n",
        "\n",
        "- RoPE (Rotary): Encodes position by rotating Q/K vectors; works great with long contexts and extrapolation.\n",
        "  - Analogy: Label each word by angle; relative positions fall out naturally.\n",
        "  - Use when: You want strong long-context behavior and compatibility with FlashAttention.\n",
        "\n",
        "- ALiBi: Adds linear bias favoring nearby tokens; simple and extrapolates well.\n",
        "  - Use when: You want simplicity and stable long-context generalization.\n",
        "\n",
        "Example: In “Alice met Bob in Paris,” positions help map “met” to “Alice” and “Bob” correctly.\n",
        "\n",
        "***\n",
        "\n",
        "## Advanced Normalization\n",
        "Stabilize training and improve speed.\n",
        "\n",
        "- RMSNorm: Normalizes by RMS magnitude, not mean+variance like LayerNorm.\n",
        "  - Why: Slightly cheaper and often just as stable; widely used in LLaMA.\n",
        "- LayerNorm axis: Choose which dimensions to normalize across—affects stability/speed on certain shapes.\n",
        "- Pre-LN vs Post-LN:\n",
        "  - Pre-LN (norm before sublayer): More stable in deep models, faster to converge.\n",
        "  - Post-LN (norm after): Original Transformer style, sometimes better final quality but trickier to train.\n",
        "- GroupNorm: Niche for conv-like patterns; rarely used in vanilla LLMs.\n",
        "\n",
        "***\n",
        "\n",
        "## Parameter-Efficient Fine-Tuning (PEFT)\n",
        "Get domain/task gains by training a tiny subset of weights.\n",
        "\n",
        "- LoRA: Inject small low-rank matrices into attention/MLP weights; only train these.\n",
        "  - Analogy: Add small “adapters” rather than rewiring the whole brain.\n",
        "  - Use when: You have limited compute or want many task variants.\n",
        "\n",
        "- AdaLoRA: Dynamically adjusts LoRA rank per layer.\n",
        "  - Why: Spend capacity where it matters most.\n",
        "\n",
        "- Adapters / Prefix Tuning:\n",
        "  - Adapters: Tiny bottleneck layers added between blocks.\n",
        "  - Prefix Tuning: Learn “virtual tokens” prepended to the sequence to steer behavior.\n",
        "  - Use when: You need multi-task variants with quick swaps.\n",
        "\n",
        "Example: Fine-tune a general LLM for finance Q&A using LoRA modules. Keep a separate LoRA for medical Q&A. Swap modules at runtime.\n",
        "\n",
        "***\n",
        "\n",
        "## Memory Optimization\n",
        "\n",
        "- Gradient Checkpointing: Recompute certain activations during backward pass instead of storing them.\n",
        "  - Why: Fit longer sequences/bigger models on the same GPU.\n",
        "  - Trade-off: Saves memory, costs extra compute.\n",
        "  - Methods:\n",
        "    - Function-based: Wrap specific blocks in checkpoint().\n",
        "    - Module-based: Checkpoint whole submodules.\n",
        "    - Sequential: Apply across the transformer stack.\n",
        "    - Selective: Only checkpoint the memory hogs (e.g., attention).\n",
        "\n",
        "- Mixed Precision + Grad Scaling:\n",
        "  - Use float16/bfloat16 to speed up and reduce memory.\n",
        "  - Automatic loss scaling prevents underflow.\n",
        "\n",
        "Example: Training 8k context on a 24GB GPU—turn on checkpointing for attention/MLP, use bfloat16, and accumulate gradients.\n",
        "\n",
        "***\n",
        "\n",
        "## Custom Loss & Training Techniques\n",
        "\n",
        "- Label Smoothing: Softens targets (e.g., 0.9 for correct class).\n",
        "  - Why: Reduces overconfidence, improves calibration.\n",
        "- Focal Loss: Downweights easy examples, focuses on hard ones.\n",
        "  - Use when: Class imbalance or noisy labels (e.g., toxicity classification).\n",
        "- Contrastive Loss: Pull matched pairs together, push mismatched apart.\n",
        "  - Use when: Learning embeddings for retrieval/RAG reranking.\n",
        "- Custom LR Schedulers: Warmup + cosine decay is a solid default; tweak per model size and batch.\n",
        "\n",
        "Example: For instruction tuning with noisy datasets, apply label smoothing to avoid overfitting spurious patterns.\n",
        "\n",
        "***\n",
        "\n",
        "## Advanced PyTorch Utilities\n",
        "\n",
        "- Forward Hooks: Peek at layer inputs/outputs to debug or record features.\n",
        "  - Example: Measure head-level attention entropy to detect dead heads.\n",
        "- Dynamic Freezing: Freeze early layers, train top layers/LoRA to speed fine-tuning.\n",
        "- Custom Autograd Functions: Write fused ops or custom gradients for performance.\n",
        "- Gradient Clipping: Prevents exploding gradients; clip by norm (e.g., 1.0) is common.\n",
        "\n",
        "***\n",
        "\n",
        "## Model Surgery and Weight Manipulation\n",
        "\n",
        "- Weight Initialization: Use stable schemes (e.g., scaled init for deep nets).\n",
        "- Pruning: Remove low-importance weights or heads; compress for deployment.\n",
        "- Weight Averaging (Model Soups): Average checkpoints from different runs/seeds—often yields more robust models.\n",
        "\n",
        "Example: After fine-tuning multiple seeds, average their weights to smooth out idiosyncrasies and boost generalization.\n",
        "\n",
        "***\n",
        "\n",
        "## Performance Optimization Tips\n",
        "\n",
        "- torch.compile (PyTorch 2.x):\n",
        "  - Speeds up model with graph capture and kernel fusion.\n",
        "  - Modes: default, reduce-overhead, max-autotune.\n",
        "- Scaled Dot-Product Attention API:\n",
        "  - Uses FlashAttention under the hood when available for big speed/memory wins.\n",
        "- Memory-Efficient Attention:\n",
        "  - Chunked or block-sparse strategies for long sequences.\n",
        "- Efficient Sequence Packing:\n",
        "  - Pack variable-length samples to reduce padding; boosts throughput.\n",
        "\n",
        "Example: For 4k+ context training, enable scaled_dot_product_attention, pack batches to reduce pad, use torch.compile, and checkpoint.\n",
        "\n",
        "***\n",
        "\n",
        "## Putting It Together: Practical Patterns\n",
        "\n",
        "- Fast Inference Server:\n",
        "  - Switch to GQA/MQA, quantize to INT8/FP8 if supported, enable FlashAttention, compile model.\n",
        "- Cheap Domain Adaptation:\n",
        "  - Add LoRA on attention and MLP, use label smoothing, freeze base layers, train in bfloat16 with grad accumulation.\n",
        "- Long-Context RAG:\n",
        "  - RoPE with long-context scaling, custom attention bias for recency via FlexAttention, checkpoint attention, use retrieval-tuned contrastive loss for the retriever.\n",
        "\n",
        "***\n",
        "\n",
        "## Tiny Code Sketches (illustrative)\n",
        "\n",
        "Multi-Query/Grouped Query idea:\n",
        "```python\n",
        "# Pseudocode style – concept only\n",
        "Q = W_q(x)          # per head\n",
        "K = shared_W_k(x)   # shared across heads (MQA)\n",
        "V = shared_W_v(x)   # shared across heads (MQA)\n",
        "# For GQA: K/V shared within each head group\n",
        "```\n",
        "\n",
        "LoRA injection:\n",
        "```python\n",
        "# W_out = W_base + A @ B, where A,B are small trainable low-rank\n",
        "W_eff = W_base + A @ B\n",
        "y = x @ W_eff\n",
        "```\n",
        "\n",
        "FlexAttention score modifier:\n",
        "```python\n",
        "# scores = (Q @ K.T)/sqrt(d) + custom_bias\n",
        "# e.g., bias recent tokens: bias[i,j] = -lambda * (i-j>0)\n",
        "```\n",
        "\n",
        "Gradient checkpointing (module-based):\n",
        "```python\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "model.blocks = checkpoint_sequential(model.blocks, chunks=4)\n",
        "```\n",
        "\n",
        "torch.compile:\n",
        "```python\n",
        "model = torch.compile(model, mode=\"max-autotune\")\n",
        "```\n",
        "\n",
        "Gradient clipping:\n",
        "```python\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## When to Choose What\n",
        "\n",
        "- Need lower latency and memory at inference: prefer GQA/MQA, FlashAttention, quantization.\n",
        "- Need better reasoning without more compute: try SwiGLU, better normalization (RMSNorm), careful LR schedule.\n",
        "- Many tasks/clients on one base model: use LoRA/adapters or prefix tuning for each task.\n",
        "- Limited GPU memory: mixed precision + checkpointing + sequence packing.\n",
        "- Retrieval-heavy systems: use contrastive losses for retriever, FlexAttention to bias context usage, RoPE for long contexts.\n",
        "\n",
        "Use this as a menu: pick a few techniques based on your bottleneck (latency, memory, quality, data). Start with LoRA + RMSNorm + RoPE + FlashAttention + torch.compile, then iterate with FlexAttention or GQA/MQA as needed."
      ],
      "metadata": {
        "id": "Kk9PaAqUzCad"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQlA07f5zFN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}